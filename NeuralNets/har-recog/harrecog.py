# -*- coding: utf-8 -*-
"""harrecog.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X-ergaw5vTNcO6lA1MgqCtVHCQ8zQd03
"""

from google.colab import drive
drive.mount('/content/drive')

cd '/content/drive/My Drive/FinalP/har-recog'

"""#####Downloading and extracting HARDataset"""

!sh get_datasets.sh

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import tensorflow as tf
import seaborn as sns

"""####Loading accelerometer data"""

import pandas as pd
import glob

path = r'wisdm-dataset/raw/phone/accel'
all_files = glob.glob(path + "/*.txt")
di = r'wisdm-dataset/activity.txt'
li = []

for filename in all_files:
    column_names = ['user-id','activity','timestamp', 'x-axis', 'y-axis', 'z-axis']
    df = pd.read_csv(filename, header = None, names = column_names)
    df['z-axis'].replace(regex=True, inplace=True, to_replace=r';', value=r'')
    li.append(df)

data_accel = pd.concat(li, axis=0, ignore_index=True)

data_accel.head(30)

data_accel.shape

path = r'wisdm-dataset/raw/phone/gyro'
all_files = glob.glob(path + "/*.txt")

li1 = []

for filename in all_files:
    column_names = ['user-id','activity','timestamp', 'x-axis', 'y-axis', 'z-axis']
    df1 = pd.read_csv(filename, header = None, names = column_names)
    df1['z-axis'].replace(regex=True, inplace=True, to_replace=r';', value=r'')
    li1.append(df1)

data_gyro = pd.concat(li1, axis=0, ignore_index=True)

data_gyro.head(30)

data_gyro.shape

"""#####Data cleaning for both sensor data"""

def convert_to_float(x):

    try:
        return np.float(x)
    except:
        return np.nan

data_accel['z-axis'] = data_accel['z-axis'].apply(convert_to_float)
data_accel.dropna(axis=0, how='any', inplace=True)
data_accel.dtypes

data_gyro['z-axis'] = data_accel['z-axis'].apply(convert_to_float)
data_gyro.dropna(axis=0, how='any', inplace=True)
data_gyro.dtypes

"""####Visualization of data"""

data_accel['activity'].value_counts().plot(kind='bar',
                                   title='Training Examples by Activity Type')
plt.show()

data_gyro['activity'].value_counts().plot(kind='bar',
                                   title='Training Examples by Activity Type')
plt.show()

plt.figure(figsize=(10,10))
data_accel['user-id'].value_counts().plot(kind='bar',
                                  title='Training Examples by User')
plt.show()

plt.figure(figsize=(10,10))
data_gyro['user-id'].value_counts().plot(kind='bar',
                                  title='Training Examples by User')
plt.show()

def plot_activity(activity, data):

    fig, (ax0, ax1, ax2) = plt.subplots(nrows=3,
         figsize=(15, 10),
         sharex=True)
    plot_axis(ax0, data['timestamp'], data['x-axis'], 'X-Axis')
    plot_axis(ax1, data['timestamp'], data['y-axis'], 'Y-Axis')
    plot_axis(ax2, data['timestamp'], data['z-axis'], 'Z-Axis')
    plt.subplots_adjust(hspace=0.2)
    fig.suptitle(activity)
    plt.subplots_adjust(top=0.90)
    plt.show()

def plot_axis(ax, x, y, title):

    ax.plot(x, y, 'r')
    ax.set_title(title)
    ax.xaxis.set_visible(False)
    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])
    ax.set_xlim([min(x), max(x)])
    ax.grid(True)

for activity in np.unique(data_accel['activity']):
    subset = data_accel[data_accel['activity'] == activity][:180]
    plot_activity(activity, subset)

for activity in np.unique(data_gyro['activity']):
    subset = data_gyro[data_gyro['activity'] == activity][:180]
    plot_activity(activity, subset)

from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn import preprocessing

"""#####Feature Engineering"""

LABEL_accel= 'ActivityEncoded'
le = preprocessing.LabelEncoder()
data_accel[LABEL_accel] = le.fit_transform(data_accel['activity'].values.ravel())

LABEL_gyro = 'ActivityEncoded'
le = preprocessing.LabelEncoder()
data_gyro[LABEL_gyro] = le.fit_transform(data_gyro['activity'].values.ravel())

accel_test = data_accel[data_accel['user-id'] > 1640]
accel_train = data_accel[data_accel['user-id'] <= 1640]
print(accel_train.shape, accel_test.shape)

gyro_test = data_gyro[data_gyro['user-id'] > 1640]
gyro_train = data_gyro[data_gyro['user-id'] <= 1640]
print(gyro_train.shape,gyro_test.shape)

pd.options.mode.chained_assignment = None  
accel_train['x-axis'] = accel_train['x-axis'] / accel_train['x-axis'].max()
accel_train['y-axis'] = accel_train['y-axis'] / accel_train['y-axis'].max()
accel_train['z-axis'] = accel_train['z-axis'] / accel_train['z-axis'].max()

accel_train = accel_train.round({'x-axis': 4, 'y-axis': 4, 'z-axis': 4})

pd.options.mode.chained_assignment = None  
accel_test['x-axis'] = accel_test['x-axis'] / accel_train['x-axis'].max()
accel_test['y-axis'] = accel_test['y-axis'] / accel_train['y-axis'].max()
accel_test['z-axis'] = accel_test['z-axis'] / accel_train['z-axis'].max()

accel_test = accel_test.round({'x-axis': 4, 'y-axis': 4, 'z-axis': 4})

pd.options.mode.chained_assignment = None  
gyro_train['x-axis'] = gyro_train['x-axis'] / accel_train['x-axis'].max()
gyro_train['y-axis'] = gyro_train['y-axis'] / accel_train['y-axis'].max()
gyro_train['z-axis'] = gyro_train['z-axis'] / accel_train['z-axis'].max()

gyro_train = gyro_train.round({'x-axis': 4, 'y-axis': 4, 'z-axis': 4})

pd.options.mode.chained_assignment = None  
gyro_test['x-axis'] = gyro_test['x-axis'] / accel_train['x-axis'].max()
gyro_test['y-axis'] = gyro_test['y-axis'] / accel_train['y-axis'].max()
gyro_test['z-axis'] = gyro_test['z-axis'] / accel_train['z-axis'].max()

gyro_test = gyro_test.round({'x-axis': 4, 'y-axis': 4, 'z-axis': 4})

TIME_PERIODS = 80

STEP_DISTANCE = 40

def create_segments_and_labels(df, time_steps, step, label_name):

    # x, y, z acceleration as features
    N_FEATURES = 3
    # Number of steps to advance in each iteration (for me, it should always
    # be equal to the time_steps in order to have no overlap between segments)
    # step = time_steps
    segments = []
    labels = []
    for i in range(0, len(df) - time_steps, step):
        xs = df['x-axis'].values[i: i + time_steps]
        ys = df['y-axis'].values[i: i + time_steps]
        zs = df['z-axis'].values[i: i + time_steps]
        # Retrieve the most often used label in this segment
        label = stats.mode(df[label_name][i: i + time_steps])[0][0]
        segments.append([xs, ys, zs])
        labels.append(label)

    # Bring the segments into a better shape
    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, time_steps, N_FEATURES)
    labels = np.asarray(labels)

    return reshaped_segments, labels

accel_x_train, accel_y_train = create_segments_and_labels(accel_train,
                                              TIME_PERIODS,
                                              STEP_DISTANCE,
                                              LABEL_accel)

accel_x_test, accel_y_test = create_segments_and_labels(accel_test,
                                              TIME_PERIODS,
                                              STEP_DISTANCE,
                                              LABEL_accel)

print('accel_x_train shape: ', accel_x_train.shape)
print(accel_x_train.shape[0], 'training samples')
print('accel_y_train shape: ', accel_y_train.shape)

print('accel_x_test shape: ', accel_x_test.shape)
print(accel_x_test.shape[0], 'training samples')
print('accel_y_test shape: ', accel_y_test.shape)

num_time_periods, num_sensors = accel_x_train.shape[1], accel_x_train.shape[2]
num_classes = le.classes_.size
print(list(le.classes_))

num_time_periods, num_sensors = accel_x_test.shape[1], accel_x_test.shape[2]
num_classes = le.classes_.size
print(list(le.classes_))

input_shape = (num_time_periods*num_sensors)
accel_x_train = accel_x_train.reshape(accel_x_train.shape[0], input_shape)
print('accel_x_train shape:', accel_x_train.shape)
print('input_shape:', input_shape)

input_shape = (num_time_periods*num_sensors)
accel_x_test = accel_x_test.reshape(accel_x_test.shape[0], input_shape)
print('accel_x_test shape:', accel_x_test.shape)
print('input_shape:', input_shape)

accel_x_train = accel_x_train.astype('float32')
accel_y_train = accel_y_train.astype('float32')

accel_x_test = accel_x_test.astype('float32')
accel_y_test = accel_y_test.astype('float32')

import keras
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense, Dropout, Flatten, Reshape
from keras.layers import Conv1D, MaxPooling1D
from keras.utils import np_utils

accel_y_train_hot = np_utils.to_categorical(accel_y_train, num_classes)
print('New accel_y_train shape: ', accel_y_train_hot.shape)

accel_y_test_hot = np_utils.to_categorical(accel_y_test, num_classes)
print('New accel_y_test shape: ', accel_y_test_hot.shape)

model_cnn_accel = Sequential()
model_cnn_accel.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(accel_x_train.shape[1], accel_x_test.shape[2])))
model_cnn_accel.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model_cnn_accel.add(Dropout(0.5))
model_cnn_accel.add(MaxPooling1D(pool_size=2))
model_cnn_accel.add(Flatten())
model_cnn_accel.add(Dense(100, activation='relu'))
model_cnn_accel.add(Dense(num_classes, activation='softmax'))
model_cnn_accel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model_cnn_accel.summary())

"""##### Model for accelerometer data"""

callbacks_list = [
    keras.callbacks.ModelCheckpoint(
        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',
        monitor='val_loss', save_best_only=True),
    keras.callbacks.EarlyStopping(monitor='acc', patience=1)
]

model_cnn_accel.compile(loss='categorical_crossentropy',
                optimizer='adam', metrics=['accuracy'])


BATCH_SIZE = 400
EPOCHS = 10


history = model_cnn_accel.fit(accel_x_train,
                      accel_y_train_hot,
                      batch_size=BATCH_SIZE,
                      epochs=EPOCHS,
                      callbacks=callbacks_list,
                      validation_split=0.2,
                      verbose=1)

def show_confusion_matrix(validations, predictions):

    matrix = metrics.confusion_matrix(validations, predictions)
    plt.figure(figsize=(10, 10))
    sns.heatmap(matrix,
                cmap='coolwarm',
                linecolor='white',
                linewidths=1,
                xticklabels=le.classes_,
                yticklabels=le.classes_,
                annot=True,
                fmt='d')
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

accel_y_pred = model_cnn_accel.predict(accel_x_test)
max_accel_y_pred = np.argmax(accel_y_pred, axis=1)
max_accel_y_test = np.argmax(accel_y_test_hot, axis=1)

show_confusion_matrix(max_accel_y_test, max_accel_y_pred)

"""##### Model for Gyro sensor"""

gyro_x_train, gyro_y_train = create_segments_and_labels(gyro_train,
                                              TIME_PERIODS,
                                              STEP_DISTANCE,
                                              LABEL_gyro)

gyro_x_test, gyro_y_test = create_segments_and_labels(gyro_test,
                                              TIME_PERIODS,
                                              STEP_DISTANCE,
                                              LABEL_gyro)

print('gyro_x_train shape: ', gyro_x_train.shape)
print(gyro_x_train.shape[0], 'training samples')
print('gyro_y_train shape: ', gyro_y_train.shape)

print('gyro_x_test shape: ', gyro_x_test.shape)
print(gyro_x_test.shape[0], 'training samples')
print('gyro_y_test shape: ', gyro_y_test.shape)

num_time_periods, num_sensors = gyro_x_train.shape[1], gyro_x_train.shape[2]
num_classes = le.classes_.size
print(list(le.classes_))

num_time_periods, num_sensors = gyro_x_test.shape[1], gyro_x_test.shape[2]
num_classes = le.classes_.size
print(list(le.classes_))

input_shape = (num_time_periods*num_sensors)
gyro_x_train = gyro_x_train.reshape(gyro_x_train.shape[0], input_shape)
print('gyro_x_train shape:', gyro_x_train.shape)
print('input_shape:', input_shape)

input_shape = (num_time_periods*num_sensors)
gyro_x_test = gyro_x_test.reshape(gyro_x_test.shape[0], input_shape)
print('gyro_x_test shape:', gyro_x_test.shape)
print('input_shape:', input_shape)

gyro_x_train = gyro_x_train.astype('float32')
gyro_y_train = gyro_y_train.astype('float32')

gyro_x_test = gyro_x_test.astype('float32')
gyro_y_test = gyro_y_test.astype('float32')

gyro_y_train_hot = np_utils.to_categorical(gyro_y_train, num_classes)
print('New gyro_y_train shape: ', gyro_y_train_hot.shape)

gyro_y_test_hot = np_utils.to_categorical(gyro_y_test, num_classes)
print('New gyro_y_test shape: ', gyro_y_test_hot.shape)

model_cnn_gyro = Sequential()
model_cnn_gyro.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(gyro_x_train.shape[1], gyro_x_test.shape[2])))
model_cnn_gyro.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model_cnn_gyro.add(Dropout(0.5))
model_cnn_gyro.add(MaxPooling1D(pool_size=2))
model_cnn_gyro.add(Flatten())
model_cnn_gyro.add(Dense(100, activation='relu'))
model_cnn_gyro.add(Dense(num_classes, activation='softmax'))
model_cnn_gyro.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model_cnn_gyro.summary())

callbacks_list = [
    keras.callbacks.ModelCheckpoint(
        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',
        monitor='val_loss', save_best_only=True),
    keras.callbacks.EarlyStopping(monitor='acc', patience=1)
]

model_cnn_gyro.compile(loss='categorical_crossentropy',
                optimizer='adam', metrics=['accuracy'])


BATCH_SIZE = 400
EPOCHS = 10


history = model_cnn_gyro.fit(gyro_x_train,
                      gyro_y_train_hot,
                      batch_size=BATCH_SIZE,
                      epochs=EPOCHS,
                      callbacks=callbacks_list,
                      validation_split=0.2,
                      verbose=1)

gyro_y_pred = model_cnn_gyro.predict(gyro_x_test)
max_gyro_y_pred = np.argmax(gyro_y_pred, axis=1)
max_gyro_y_test = np.argmax(gyro_y_test_hot, axis=1)

show_confusion_matrix(max_gyro_y_pred, max_gyro_y_test)

verbose, epochs, batch_size = 0, 15, 64

model_cnn_accel = Sequential()
model_cnn_accel.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(accel_x_train.shape[1], accel_x_test.shape[2])))
model_cnn_accel.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model_cnn_accel.add(Dropout(0.5))
model_cnn_accel.add(MaxPooling1D(pool_size=2))
model_cnn_accel.add(Flatten())
model_cnn_accel.add(Dense(100, activation='relu'))
model_cnn_accel.add(Dense(num_classes, activation='softmax'))
model_cnn_accel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_cnn_accel.fit(accel_x_train, accel_y_train_hot, epochs=epochs, batch_size=batch_size, verbose=verbose)
_, score = model_cnn_accel.evaluate(accel_x_test, accel_y_test_hot, batch_size=batch_size, verbose=0)
score = score * 100.0
print('%.3f' % (score))

model_cnn_gyro = Sequential()
model_cnn_gyro.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(gyro_x_train.shape[1], gyro_x_test.shape[2])))
model_cnn_gyro.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model_cnn_gyro.add(Dropout(0.5))
model_cnn_gyro.add(MaxPooling1D(pool_size=2))
model_cnn_gyro.add(Flatten())
model_cnn_gyro.add(Dense(100, activation='relu'))
model_cnn_gyro.add(Dense(num_classes, activation='softmax'))
model_cnn_gyro.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_cnn_gyro.fit(gyro_x_train, gyro_y_train_hot, epochs=epochs, batch_size=batch_size, verbose=verbose)
_, score = model_cnn_gyro.evaluate(gyro_x_test, gyro_y_test_hot, batch_size=batch_size, verbose=0)
score = score * 100.0
print('%.3f' % (score))

model_rnn_accel = Sequential()
model_rnn_accel.add(LSTM(100, input_shape=(accel_x_train.shape[1], accel_x_test.shape[2])))
model_rnn_accel.add(Dropout(0.5))
model_rnn_accel.add(Dense(100, activation='relu'))
model_rnn_accel.add(Dense(num_classes, activation='softmax'))
model_rnn_accel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_rnn_accel.fit(accel_x_train, accel_y_train_hot, epochs=epochs, batch_size=batch_size, verbose=verbose)
_, score= model_rnn_accel.evaluate(accel_x_test, accel_y_test_hot, batch_size=batch_size, verbose=0)
score = score * 100.0
print('%.3f' % (score))

model_rnn_gyro = Sequential()
model_rnn_gyro.add(LSTM(100, input_shape=(gyro_x_train.shape[1], gyro_x_test.shape[2])))
model_rnn_gyro.add(Dropout(0.5))
model_rnn_gyro.add(Dense(100, activation='relu'))
model_rnn_gyro.add(Dense(num_classes, activation='softmax'))
model_rnn_gyro.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_rnn_gyro.fit(gyro_x_train, gyro_y_train_hot, epochs=epochs, batch_size=batch_size, verbose=verbose)
_, score = model_rnn_gyro.evaluate(gyro_x_test, gyro_y_test_hot, batch_size=batch_size, verbose=0)
score = score * 100.0
print('%.3f' % (score))

model_dfn_accel = Sequential()
model_dfn_accel.add(Reshape((TIME_PERIODS, 3), input_shape=(accel_x_train.shape[1], accel_x_test.shape[2])))
model_dfn_accel.add(Dense(100, activation='relu'))
model_dfn_accel.add(Dense(100, activation='relu'))
model_dfn_accel.add(Dense(100, activation='relu'))
model_dfn_accel.add(Flatten())
model_dfn_accel.add(Dense(num_classes, activation='softmax'))
model_dfn_accel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_dfn_accel.fit(accel_x_train, accel_y_train_hot, epochs=epochs, batch_size=batch_size, verbose=verbose)
_, score = model_dfn_accel.evaluate(accel_x_test, accel_y_test_hot, batch_size=batch_size, verbose=0)
score = score * 100.0
print('%.3f' % (score))

model_dfn_gyro = Sequential()
model_dfn_gyro.add(Reshape((TIME_PERIODS, 3), input_shape=(gyro_x_train.shape[1], gyro_x_test.shape[2])))
model_dfn_gyro.add(Dense(100, activation='relu'))
model_dfn_gyro.add(Dense(100, activation='relu'))
model_dfn_gyro.add(Dense(100, activation='relu'))
model_dfn_gyro.add(Flatten())
model_dfn_gyro.add(Dense(num_classes, activation='softmax'))
model_dfn_gyro.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_dfn_gyro.fit(gyro_x_train, gyro_y_train_hot, epochs=epochs, batch_size=batch_size, verbose=verbose)
_, score = model_dfn_gyro.evaluate(gyro_x_test, gyro_y_test_hot, batch_size=batch_size, verbose=0)
score = score * 100.0
print('%.3f' % (score))